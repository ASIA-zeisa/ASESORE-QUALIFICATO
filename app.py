import os
import base64
import requests
from flask import Flask, request, render_template_string
from pinecone import Pinecone
from openai import OpenAI
from dotenv import load_dotenv

# â”€â”€â”€ 0) Load env vars â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
PINECONE_API_KEY   = os.getenv("PINECONE_API_KEY")
PINECONE_ENV       = os.getenv("PINECONE_ENV")
PINECONE_INDEX     = os.getenv("PINECONE_INDEX")
OPENAI_API_KEY     = os.getenv("OPENAI_API_KEY")
CUSTOM_GPT_MODEL   = os.getenv("CUSTOM_GPT_MODEL")   # p.ej. "gpt-4o-tu-PAA-completo"

# â”€â”€â”€ 0.5) Activation config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EXAM_CONFIG     = {i: 'off' for i in range(1, 61)}
EXAM_CONFIG.update({1: 'on', 2: 'on', 3: 'off', 4: 'off', 5: 'off'})
SECTION_OPTIONS = ['Lectura', 'RedacciÃ³n', 'MatemÃ¡ticas', 'Variable']

# â”€â”€â”€ 0.7) Dummy vector for filter-only queries â€” must match index dimensions
DUMMY_VECTOR = [0.0] * 1536

# â”€â”€â”€ 1) Init Pinecone & OpenAI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pc     = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)
index  = pc.Index(PINECONE_INDEX)
client = OpenAI(api_key=OPENAI_API_KEY)
app    = Flask(__name__)

# â”€â”€â”€ 2) HTML + MathJax setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HTML = '''â€¦ (sin cambios) â€¦'''  # mantenemos tu HTML tal cual

# â”€â”€â”€ 3) Home route â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route('/', methods=['GET'])
def home():
    return render_template_string(
        HTML,
        exam_config     = EXAM_CONFIG,
        section_options = SECTION_OPTIONS
    )

# â”€â”€â”€ 4) Handle question â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route('/preguntar', methods=['POST'])
def preguntar():
    texto        = (request.form.get('texto') or "").strip()
    examen       = request.form.get('examen')
    seccion      = request.form.get('seccion')
    pregunta_num = request.form.get('pregunta')
    image_file   = request.files.get('image')

    # 1) Validaciones bÃ¡sicas
    if texto and (examen or seccion or pregunta_num or image_file):
        return (
            "Si escribes tu pregunta, no puedes usar â€œExamenâ€, â€œSecciÃ³nâ€, "
            "â€œPreguntaâ€ ni subir imagen al mismo tiempo."
        ), 400
    if not (texto or examen or seccion or pregunta_num or image_file):
        return (
            "Proporciona texto, selecciona examen/secciÃ³n/pregunta o sube una imagen."
        ), 400
    if examen and not (seccion and pregunta_num):
        return "Cuando seleccionas examen, debes elegir secciÃ³n y pregunta.", 400

    # â”€â”€â”€ 2) Branch: examen puro â†’ GPT custom â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if examen and seccion and pregunta_num and not texto and not image_file:
        system_prompt = (
            "Eres un profesor de matemÃ¡ticas experto en PAA que tiene TODAS las "
            "preguntas y respuestas de todos los exÃ¡menes. Cuando te den Examen X, "
            "SecciÃ³n Y, Pregunta Z, reproduces la pregunta exacta y la explicas en 5 "
            "pasos numerados usando delimitadores \\(â€¦\\)."
        )
        user_prompt = (
            f"Examen {examen}, SecciÃ³n {seccion}, Pregunta {pregunta_num}.\n"
            "Por favor, muestra la pregunta y da la explicaciÃ³n en 5 pasos."
        )
        chat = client.chat.completions.create(
            model=CUSTOM_GPT_MODEL,
            messages=[
                {"role":"system", "content":system_prompt},
                {"role":"user",   "content":user_prompt}
            ]
        )
        # Devolvemos directamente lo que genere el custom GPT
        return chat.choices[0].message.content.strip()

    # â”€â”€â”€ 3) Branch: texto libre o imagen â†’ Pinecone + embed + formatter â”€â”€â”€â”€

    # 3a) Intentamos exact-match por metadata (opcional; puedes omitir si ya no lo usas)
    snippet = None
    if examen and seccion and pregunta_num:
        try:
            pine = index.query(
                vector=DUMMY_VECTOR,
                top_k=1,
                include_metadata=True,
                filter={
                    "exam":     int(examen),
                    "section":  seccion,
                    "question": int(pregunta_num)
                }
            )
            if pine.matches:
                meta    = pine.matches[0].metadata
                snippet = meta.get("text") or meta.get("answer")
        except Exception:
            snippet = None

    # 3b) Si hay snippet, usamos gpt-4o-mini para explicar de forma concisa
    if snippet:
        clean = snippet.strip('$')
        system_prompt = (
            "Eres un profesor de matemÃ¡ticas que explica de forma muy concisa "
            "en espaÃ±ol, en no mÃ¡s de 5 pasos numerados, usando delimitadores "
            "\\(â€¦\\) para las expresiones matemÃ¡ticas."
        )
        context = f"Examen {examen}, SecciÃ³n {seccion}, Pregunta {pregunta_num}"
        user_prompt = (
            f"EcuaciÃ³n: {context}\n"
            f"Respuesta: \\({clean}\\)\n\n"
            "Proporciona una lista numerada (1â€“5) de los pasos clave "
            "para completar el cuadrado rÃ¡pidamente."
        )
        chat = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role":"system","content":system_prompt},
                {"role":"user","content":user_prompt}
            ]
        )
        explanation = chat.choices[0].message.content.strip()
        formatted_list = (
            f"<ol><li>\\({clean}\\)</li></ol>"
            f"<p><strong>Pasos rÃ¡pidos:</strong></p>"
            f"{explanation}"
        )

    else:
        # 3c) Fallback embedding (texto o imagen) â†’ bÃºsqueda â†’ formatter
        try:
            if image_file and not texto:
                img_bytes = image_file.read()
                emb = client.embeddings.create(
                    model="image-embedding-001",
                    input=base64.b64encode(img_bytes).decode()
                )
            else:
                emb = client.embeddings.create(
                    model="text-embedding-3-small",
                    input=texto
                )
            vector = emb.data[0].embedding
            pine = index.query(
                vector=vector,
                top_k=5,
                include_metadata=True
            )
            raw_steps = [
                m.metadata.get('text') or m.metadata.get('answer')
                for m in pine.matches
                if m.metadata.get('text') or m.metadata.get('answer')
            ]
        except Exception:
            raw_steps = []

        if not raw_steps:
            try:
                wiki = requests.get(
                    'https://es.wikipedia.org/api/rest_v1/page/random/summary',
                    timeout=5
                ).json()
                raw_steps = [wiki.get('extract','Lo siento, nada')]
            except:
                return 'No hay datos en Pinecone y fallÃ³ la bÃºsqueda aleatoria.', 500

        format_msg = (
            'Eres un formateador HTML muy estricto. Toma estas frases y devuÃ©lvelas '
            'como una lista ordenada (<ol><li>â€¦</li></ol>) en espaÃ±ol, sin texto '
            'adicional. Usa siempre los delimitadores LaTeX \\(â€¦\\) para las fÃ³rmulas.\n\n'
            + '\n'.join(f'- {s}' for s in raw_steps)
        )
        try:
            chat = client.chat.completions.create(
                model='gpt-4o-mini',
                messages=[
                    {'role':'system','content':format_msg},
                    {'role':'user','content':'Por favor formatea la lista.'}
                ]
            )
            formatted_list = chat.choices[0].message.content.strip()
        except Exception as e:
            return f'Error de formateo: {e}', 500

    formatted_list = formatted_list.replace("\\[", "\\(").replace("\\]", "\\)")

    # 4) Montaje final de la respuesta para texto libre / imagen
    response_fragment = (
        f"<p><strong>Enunciado:</strong> {texto}</p>"
        f"<p><strong>Examen:</strong> {examen}</p>"
        f"<p><strong>SecciÃ³n:</strong> {seccion}</p>"
        f"<p><strong>Pregunta nÂº:</strong> {pregunta_num}</p>"
        f"{formatted_list} ğŸ¤Œ"
    )
    return response_fragment

# â”€â”€â”€ 5) Run server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == '__main__':
    app.run(
        host='0.0.0.0',
        port=int(os.getenv('PORT','8000')),
        debug=False
    )
